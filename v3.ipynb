{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "–ê–Ω–∞–ª–∏—Ç–∏–∫–∞ —Å–ª–µ–¥—É—é—â–∞—è:\n",
    "* –°–ª–∞–±—ã–µ –º–µ—Å—Ç–∞ (–∫—É–¥–∞ —É—Ö–æ–¥–∏—Ç –±–æ–ª—å—à–µ –≤—Å–µ–≥–æ)\n",
    "    - –¢–æ–ø –∫–∞—Ç–µ–≥–æ—Ä–∏–π –ø–æ —Ä–∞—Å—Ö–æ–¥–∞–º\n",
    "    - –î–æ–ª–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–π\n",
    "    - –í—Å–ø–ª–µ—Å–∫–∏ —Ä–∞—Å—Ö–æ–¥–æ–≤ –ø–æ –¥–Ω—è–º (rule-based + ML anomaly)\n",
    "* –†–µ–≥—É–ª—è—Ä–Ω—ã–µ –ø–ª–∞—Ç–µ–∂–∏\n",
    "    - –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º\n",
    "    - –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–µ—Ä–∏–æ–¥–∏—á–Ω–æ—Å—Ç—å (weekly, monthly)\n",
    "    - –ò—Ç–æ–≥: —Å–ø–∏—Å–æ–∫ \"–ø–æ—Ö–æ–∂–µ –Ω–∞ –ø–æ–¥–ø–∏—Å–∫—É/—Ä–µ–≥—É–ª—è—Ä–Ω—ã–π –ø–ª–∞—Ç—ë–∂\"\n",
    "* –≠–∫–æ–Ω–æ–º–∏—è –∏ \"—Å–∫–æ–ª—å–∫–æ –º–æ–∂–Ω–æ –æ—Ç–ª–æ–∂–∏—Ç—å\"\n",
    "    - –ü–æ–ª–∏—Ç–∏–∫–∞ —ç–∫–æ–Ω–æ–º–∏–∏ (–ø—Ä–∞–≤–∏–ª–∞ –∏ –º–æ–¥–µ–ª—å)\n",
    "    - –†–∞—Å—á—ë—Ç `potential_savings_total`\n",
    "    - –†–∞–∑–ª–æ–∂–µ–Ω–∏–µ —ç–∫–æ–Ω–æ–º–∏–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º –∏ –∫–ª–∞—Å—Ç–µ—Ä–∞–º\n",
    "    - –ò—Ç–æ–≥: \"–µ—Å–ª–∏ —Å–Ω–∏–∑–∏—Ç—å X%, —Ç–æ –º–æ–∂–Ω–æ –æ—Ç–ª–æ–∂–∏—Ç—å Y —Ä—É–±–ª–µ–π –∑–∞ –ø–µ—Ä–∏–æ–¥ N\""
   ],
   "id": "e9dce3775a1dc91c"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-13T14:01:29.412968Z",
     "start_time": "2026-01-13T14:01:29.409331Z"
    }
   },
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n"
   ],
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T14:01:29.573939Z",
     "start_time": "2026-01-13T14:01:29.569324Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MCC_MAP = {\n",
    "    \"–¢–æ–ø–ª–∏–≤–æ, –ê–ó–°\": [5983, 5541, 5542],\n",
    "    \"–ê–≤—Ç–æ–º–æ–π–∫–∞\": [7542],\n",
    "\n",
    "    \"–û–¥–µ–∂–¥–∞ –∏ –æ–±—É–≤—å\": [5691, 5137, 5611, 5661, 5621, 5651],\n",
    "\n",
    "    \"–¢–∞–∫—Å–∏ –∏ –∫–∞—Ä—à–µ—Ä–∏–Ω–≥\": list(range(3351, 3442)) + [4121, 7512, 3990],\n",
    "\n",
    "    \"–§–∞—Å—Ç—Ñ—É–¥\": [5814],\n",
    "\n",
    "    \"–≠–ª–µ–∫—Ç—Ä–æ–Ω–∏–∫–∞ –∏ —Ç–µ—Ö–Ω–∏–∫–∞\": [5722, 5732, 5946],\n",
    "\n",
    "    \"–°—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã\": [5039, 5198, 5211, 5231, 5251, 5713, 5714, 5712, 5074],\n",
    "    \"–¢–æ–≤–∞—Ä—ã –¥–ª—è –¥–æ–º–∞\": [5200, 5718, 5719, 5021, 5261],\n",
    "\n",
    "    \"–ê–≤—Ç–æ—Å–µ—Ä–≤–∏—Å—ã –∏ –∑–∞–ø—á–∞—Å—Ç–∏\": [7531, 7534, 7535, 7538, 5511, 5521, 5532, 5533],\n",
    "\n",
    "    \"–°–∞–ª–æ–Ω—ã –∫—Ä–∞—Å–æ—Ç—ã –∏ –°–ü–ê\": [7230, 7297, 7298],\n",
    "\n",
    "    \"–°–ø–æ—Ä—Ç–∏–≤–Ω—ã–µ –º–∞–≥–∞–∑–∏–Ω—ã\": [5655, 5940, 5941],\n",
    "\n",
    "    \"–§–∏—Ç–Ω–µ—Å\": [7941, 7997],\n",
    "\n",
    "    \"–¢—É—Ä–∞–≥–µ–Ω—Ç—Å—Ç–≤–∞\": [5962, 4722, 4723, 7032, 7033, 4411],\n",
    "\n",
    "    \"–û—Ç–µ–ª–∏\": list(range(3501, 3990)) + list(range(3991, 4000)) + [7011],\n",
    "\n",
    "    \"–ê–≤–∏–∞–±–∏–ª–µ—Ç—ã –∏ –ñ–î –±–∏–ª–µ—Ç—ã\": [\n",
    "        *list(range(3000, 3302)), 4511, 4582, 4304, 4415, 4418, 4011, 4112\n",
    "    ],\n",
    "\n",
    "    \"–ê–ø—Ç–µ–∫–∏\": [5122, 5912],\n",
    "\n",
    "    \"–ú–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ —É—Å–ª—É–≥–∏\": [\n",
    "        8011, 5976, 8031, 8042, 8043, 8049, 8050, 8062, 8021, 8071\n",
    "    ],\n",
    "\n",
    "    \"–†–µ—Å—Ç–æ—Ä–∞–Ω—ã –∏ –∫–∞—Ñ–µ\": [5811, 5812, 5813],\n",
    "\n",
    "    \"–¢–µ–∞—Ç—Ä—ã, –º—É–∑–µ–∏ –∏ –≤—ã—Å—Ç–∞–≤–∫–∏\": [7922, 7991, 7996, 7998, 7999],\n",
    "\n",
    "    \"–ú–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å—ã\": [5262, 5300, 7278],\n",
    "\n",
    "    \"–û–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ\": [8211, 8220, 8241, 8244, 8249, 8299, 8351],\n",
    "\n",
    "    \"–î–µ—Ç—Å–∫–∏–µ —Ç–æ–≤–∞—Ä—ã\": [5641, 5945],\n",
    "\n",
    "    \"–ö–æ—Å–º–µ—Ç–∏–∫–∞\": [5977],\n",
    "\n",
    "    \"–ö–Ω–∏–≥–∏\": [2741, 5192, 5942, 5994],\n",
    "\n",
    "    \"–¶–≤–µ—Ç—ã\": [5193, 5992],\n",
    "\n",
    "    \"–ö–∏–Ω–æ\": [7829, 7832, 7841],\n",
    "\n",
    "    \"–ö–∞–Ω—Ü—Ç–æ–≤–∞—Ä—ã\": [5111, 5943],\n",
    "\n",
    "    \"–¢–æ–≤–∞—Ä—ã –¥–ª—è –∂–∏–≤–æ—Ç–Ω—ã—Ö\": [742, 5995],  # 0742 -> 742\n",
    "\n",
    "    \"–ë—ã—Ç–æ–≤—ã–µ —É—Å–ª—É–≥–∏\": [\n",
    "        7210, 7211, 7216, 7217, 7251, 7349, 7379, 7622, 7623, 7629, 7641, 7692, 7699\n",
    "    ],\n",
    "\n",
    "    \"–°–≤—è–∑—å, –∏–Ω—Ç–µ—Ä–Ω–µ—Ç, –¢–í\": [4899, 4813, 4815, 4821, 4816, 4814],\n",
    "}\n"
   ],
   "id": "e22722773af962b4",
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T14:01:29.720739Z",
     "start_time": "2026-01-13T14:01:29.714335Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def _find_header_row(df_raw: pd.DataFrame, required_cols: List[str], max_scan: int = 200) -> int:\n",
    "    \"\"\"Finds header row index in messy exports.\n",
    "\n",
    "    Args:\n",
    "        df_raw: Raw dataframe without header.\n",
    "        required_cols: Keywords that should appear in header row.\n",
    "        max_scan: Maximum rows to scan.\n",
    "\n",
    "    Returns:\n",
    "        Header row index.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If header row can't be detected.\n",
    "    \"\"\"\n",
    "    best_i, best_score = 0, -1\n",
    "    for i in range(min(len(df_raw), max_scan)):\n",
    "        row = df_raw.iloc[i].astype(str).str.lower().tolist()\n",
    "        score = 0\n",
    "        for col_kw in required_cols:\n",
    "            if any(col_kw.lower() in cell for cell in row):\n",
    "                score += 1\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_i = i\n",
    "        if score >= max(1, len(required_cols) - 1):\n",
    "            return i\n",
    "    if best_score <= 0:\n",
    "        raise ValueError(\"–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Å—Ç—Ä–æ–∫—É –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –≤ —Ñ–∞–π–ª–µ.\")\n",
    "    return best_i\n",
    "\n",
    "\n",
    "def load_bank_file(path: str) -> pd.DataFrame:\n",
    "    \"\"\"Loads bank export from CSV or XLSX.\n",
    "\n",
    "    Args:\n",
    "        path: Path to file.\n",
    "\n",
    "    Returns:\n",
    "        Raw dataframe as read from file.\n",
    "    \"\"\"\n",
    "    if path.lower().endswith(\".csv\"):\n",
    "        # –±–∞–Ω–∫–∏ —á–∞—Å—Ç–æ –≤ cp1251 / utf-8-sig\n",
    "        for enc in (\"utf-8-sig\", \"cp1251\", \"utf-8\"):\n",
    "            try:\n",
    "                return pd.read_csv(path, encoding=enc)\n",
    "            except Exception:\n",
    "                continue\n",
    "        return pd.read_csv(path)  # fallback\n",
    "    elif path.lower().endswith((\".xlsx\", \".xls\")):\n",
    "        df_raw = pd.read_excel(path, header=None)\n",
    "        header_row = _find_header_row(df_raw, required_cols=[\"–¥–∞—Ç–∞\", \"—Å—É–º\", \"–æ–ø–∏—Å\", \"–∫–∞—Ç–µ–≥\"])\n",
    "        return pd.read_excel(path, header=header_row)\n",
    "    else:\n",
    "        raise ValueError(\"–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è —Ç–æ–ª—å–∫–æ CSV –∏ XLSX.\")\n",
    "\n",
    "\n",
    "def extract_mcc(text: str) -> Optional[int]:\n",
    "    \"\"\"Extracts MCC code from description text if present.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return None\n",
    "    m = re.search(r\"mcc[:\\s]*([0-9]{4})\", text.lower())\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "\n",
    "def normalize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Normalizes raw bank dataframe to standard schema.\n",
    "\n",
    "    Output columns:\n",
    "        date, description, amount, bank_category, mcc\n",
    "\n",
    "    Args:\n",
    "        df: Raw dataframe.\n",
    "\n",
    "    Returns:\n",
    "        Normalized dataframe.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If required columns can't be mapped.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    cols = {c: str(c).lower() for c in df.columns}\n",
    "\n",
    "    # —ç–≤—Ä–∏—Å—Ç–∏–∫–∏ –º–∞–ø–ø–∏–Ω–≥–∞\n",
    "    date_cols = [c for c, n in cols.items() if \"–¥–∞—Ç–∞\" in n or \"date\" in n]\n",
    "    desc_cols = [c for c, n in cols.items() if \"–æ–ø–∏—Å\" in n or \"–Ω–∞–∑–Ω–∞—á\" in n or \"description\" in n]\n",
    "    amt_cols = [c for c, n in cols.items() if \"—Å—É–º\" in n or \"amount\" in n or \"–∏—Ç–æ–≥\" in n]\n",
    "    cat_cols = [c for c, n in cols.items() if \"–∫–∞—Ç–µ–≥\" in n or \"category\" in n]\n",
    "\n",
    "    if not date_cols or not desc_cols or not amt_cols:\n",
    "        raise ValueError(\"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–ª—é—á–µ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏: –¥–∞—Ç–∞/–æ–ø–∏—Å–∞–Ω–∏–µ/—Å—É–º–º–∞.\")\n",
    "\n",
    "    df = df.rename(columns={\n",
    "        date_cols[0]: \"date\",\n",
    "        desc_cols[0]: \"description\",\n",
    "        amt_cols[0]: \"amount\",\n",
    "        (cat_cols[0] if cat_cols else None): \"bank_category\",\n",
    "    })\n",
    "\n",
    "    if \"bank_category\" not in df.columns:\n",
    "        df[\"bank_category\"] = \"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ\"\n",
    "\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\", dayfirst=True)\n",
    "    df[\"amount\"] = pd.to_numeric(df[\"amount\"], errors=\"coerce\")\n",
    "    df[\"description\"] = df[\"description\"].astype(str)\n",
    "    df[\"bank_category\"] = df[\"bank_category\"].astype(str)\n",
    "\n",
    "    df = df.dropna(subset=[\"date\", \"amount\"]).reset_index(drop=True)\n",
    "    df[\"mcc\"] = df[\"description\"].apply(extract_mcc)\n",
    "\n",
    "    return df\n"
   ],
   "id": "8df56f07b04d340d",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ML —Ç–∏–ø –æ–ø–µ—Ä–∞—Ü–∏–∏: income / transfer / spending",
   "id": "888f99e048ed7bb7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T14:01:30.002061Z",
     "start_time": "2026-01-13T14:01:29.996054Z"
    }
   },
   "cell_type": "code",
   "source": [
    "TRANSFER_KWS = [\n",
    "    \"—Å–±–ø\", \"–ø–µ—Ä–µ–≤–æ–¥\", \"–º–µ–∂–¥—É —Å—á–µ—Ç\", \"–≤–Ω—É—Ç—Ä–∏–±–∞–Ω–∫–æ–≤\", \"–ø–æ–ø–æ–ª–Ω–µ–Ω–∏–µ\", \"–∫–æ–ø–∏–ª–∫–∞\",\n",
    "    \"–ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏–µ\", \"–ø–µ—Ä–µ–≤–æ–¥ —Å—Ä–µ–¥—Å—Ç–≤\", \"p2p\", \"card2card\"\n",
    "]\n",
    "INCOME_KWS = [\n",
    "    \"–∑–∞—á–∏—Å–ª–µ–Ω–∏–µ\", \"–∑–∞—Ä–ø–ª–∞—Ç\", \"salary\", \"–≤–æ–∑–≤—Ä–∞—Ç\", \"refund\", \"–∫—ç—à–±—ç–∫\", \"cashback\",\n",
    "    \"–ø–æ—Å–æ–±–∏–µ\", \"—Å—Ç–∏–ø–µ–Ω–¥\", \"–ø–µ–Ω—Å–∏\", \"–Ω–∞—á–∏—Å–ª–µ–Ω–∏–µ\"\n",
    "]\n",
    "\n",
    "REFUND_KWS = [\"–≤–æ–∑–≤—Ä–∞—Ç\", \"refund\", \"–æ—Ç–º–µ–Ω–∞\", \"reversal\", \"—Å—Ç–æ—Ä–Ω–æ\"]\n",
    "\n",
    "EMPLOYER_KWS = [\n",
    "    \"–æ–æ–æ\", \"–∑–∞–æ\", \"–ø–∞–æ\", \"–∏–ø\", \"ltd\", \"llc\", \"inc\",\n",
    "    \"—Ä–∞–±–æ—Ç–æ–¥–∞—Ç–µ–ª—å\", \"salary\", \"–∑–∞—Ä–∞–±–æ—Ç\"\n",
    "]\n",
    "\n",
    "SELF_TRANSFER_KWS = [\n",
    "    \"–º–µ–∂–¥—É —Å—á–µ—Ç\", \"—Å–≤–æ–∏–º–∏\", \"—Å–µ–±–µ\", \"–º–æ–π —Å—á–µ—Ç\", \"–≤–Ω—É—Ç—Ä–∏–±–∞–Ω–∫–æ–≤\",\n",
    "    \"—Å–æ —Å—á–µ—Ç–∞\", \"–Ω–∞ —Å—á–µ—Ç\"\n",
    "]\n",
    "\n",
    "\n",
    "def weak_label_transaction_type(amount: float, description: str) -> Optional[str]:\n",
    "    desc = (description or \"\").lower()\n",
    "\n",
    "    # 1Ô∏è‚É£ –í–æ–∑–≤—Ä–∞—Ç—ã\n",
    "    if any(k in desc for k in REFUND_KWS):\n",
    "        return \"refund\"\n",
    "\n",
    "    # 2Ô∏è‚É£ –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –ø–µ—Ä–µ–≤–æ–¥—ã (–ù–ï –¥–æ—Ö–æ–¥)\n",
    "    if any(k in desc for k in SELF_TRANSFER_KWS):\n",
    "        return \"transfer\"\n",
    "\n",
    "    # 3Ô∏è‚É£ –ó–∞—Ä–ø–ª–∞—Ç—ã, –≤—ã–ø–ª–∞—Ç—ã –æ—Ç –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π\n",
    "    if any(k in desc for k in EMPLOYER_KWS):\n",
    "        return \"income\"\n",
    "\n",
    "    # 4Ô∏è‚É£ –Ø–≤–Ω—ã–µ –ø–µ—Ä–µ–≤–æ–¥—ã p2p\n",
    "    if \"—Å–±–ø\" in desc or \"p2p\" in desc:\n",
    "        if amount > 0:\n",
    "            return \"income\"  # –≤—Ö–æ–¥—è—â–∏–π p2p = –¥–æ—Ö–æ–¥\n",
    "        else:\n",
    "            return \"transfer\"\n",
    "\n",
    "    # 5Ô∏è‚É£ –û–±—â–∏–µ —Å–ª–æ–≤–∞ –ø–µ—Ä–µ–≤–æ–¥–∞\n",
    "    if \"–ø–µ—Ä–µ–≤–æ–¥\" in desc or \"–ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏–µ\" in desc:\n",
    "        if amount > 0:\n",
    "            return \"income\"\n",
    "        else:\n",
    "            return \"transfer\"\n",
    "\n",
    "    # 6Ô∏è‚É£ Spending\n",
    "    if amount < 0:\n",
    "        return \"spending\"\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def train_transaction_type_model(df: pd.DataFrame) -> Pipeline:\n",
    "    \"\"\"Trains transaction type classifier.\n",
    "\n",
    "    Uses weak labels for confident samples only.\n",
    "\n",
    "    Args:\n",
    "        df: Normalized dataframe.\n",
    "\n",
    "    Returns:\n",
    "        sklearn Pipeline that predicts transaction_type.\n",
    "    \"\"\"\n",
    "    work = df.copy()\n",
    "    work[\"weak_type\"] = [\n",
    "        weak_label_transaction_type(a, d) for a, d in zip(work[\"amount\"], work[\"description\"])\n",
    "    ]\n",
    "    train_df = work.dropna(subset=[\"weak_type\"]).copy()\n",
    "\n",
    "    # —É–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–∏, –≥–¥–µ positive amount –∏ transfer ‚Äî —ç—Ç–æ —á–∞—Å—Ç–æ –≥—Ä—è–∑–Ω—ã–µ –ª–µ–π–±–ª—ã\n",
    "    train_df = train_df[\n",
    "        ~((train_df[\"weak_type\"] == \"transfer\") & (train_df[\"amount\"] > 0))\n",
    "    ]\n",
    "\n",
    "    # –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –º–∞–ª–æ ‚Äî –º–æ–¥–µ–ª—å –±—É–¥–µ—Ç –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–∞, –Ω–æ –≤—Å—ë —Ä–∞–≤–Ω–æ –ª—É—á—à–µ –ø—Ä–∞–≤–∏–ª.\n",
    "    X = train_df[[\"description\", \"amount\"]]\n",
    "    y = train_df[\"weak_type\"]\n",
    "\n",
    "    # –ü—Ä–∏–∑–Ω–∞–∫–∏: TF-IDF —Ç–µ–∫—Å—Ç–∞ + –∑–Ω–∞–∫ —Å—É–º–º—ã\n",
    "    pre = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"text\", TfidfVectorizer(max_features=8000, ngram_range=(1, 2), min_df=2),\n",
    "             \"description\"),\n",
    "            (\"num\", Pipeline([(\"scaler\", StandardScaler())]), [\"amount\"]),\n",
    "        ],\n",
    "        remainder=\"drop\"\n",
    "    )\n",
    "\n",
    "    clf = LogisticRegression(max_iter=2000, class_weight=\"balanced\")\n",
    "    model = Pipeline([(\"pre\", pre), (\"clf\", clf)])\n",
    "\n",
    "    model.fit(X, y)\n",
    "    return model\n",
    "\n",
    "\n",
    "def apply_transaction_type_model(df: pd.DataFrame, model: Pipeline) -> pd.DataFrame:\n",
    "    \"\"\"Adds transaction_type prediction to dataframe.\"\"\"\n",
    "    out = df.copy()\n",
    "    out[\"transaction_type\"] = model.predict(out[[\"description\", \"amount\"]])\n",
    "    return out\n"
   ],
   "id": "83334b0cd9df53d0",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è —Ä–∞—Å—Ö–æ–¥–æ–≤",
   "id": "e82beb342ca4e1a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T14:01:30.267898Z",
     "start_time": "2026-01-13T14:01:30.262358Z"
    }
   },
   "cell_type": "code",
   "source": [
    "OTHER_BANK_LABELS = {\"–ø—Ä–æ—á–∏–µ –æ–ø–µ—Ä–∞—Ü–∏–∏\", \"–ø—Ä–æ—á–µ–µ\", \"–¥—Ä—É–≥–æ–µ\", \"–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ\", \"\"}\n",
    "\n",
    "\n",
    "def mcc_to_category(mcc: Optional[int]) -> Optional[str]:\n",
    "    \"\"\"Maps MCC to human category if possible.\"\"\"\n",
    "    if mcc is None:\n",
    "        return None\n",
    "    for cat, codes in MCC_MAP.items():\n",
    "        if mcc in codes:\n",
    "            return cat\n",
    "    return None\n",
    "\n",
    "\n",
    "def build_pseudo_category(df_spending: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Builds pseudo labels for category training.\n",
    "\n",
    "    Args:\n",
    "        df_spending: Dataframe with only spending transactions.\n",
    "\n",
    "    Returns:\n",
    "        Dataframe with new column pseudo_category.\n",
    "    \"\"\"\n",
    "    out = df_spending.copy()\n",
    "    bank = out[\"bank_category\"].astype(str).str.lower().str.strip()\n",
    "    out[\"mcc_category\"] = out[\"mcc\"].apply(mcc_to_category)\n",
    "\n",
    "    out[\"pseudo_category\"] = np.where(\n",
    "        ~bank.isin(OTHER_BANK_LABELS),\n",
    "        out[\"bank_category\"],\n",
    "        np.where(out[\"mcc_category\"].notna(), out[\"mcc_category\"], \"–î—Ä—É–≥–æ–µ\")\n",
    "    )\n",
    "    return out\n",
    "\n",
    "\n",
    "def train_spending_category_model(df_spending: pd.DataFrame) -> Pipeline:\n",
    "    \"\"\"Trains spending category classifier on pseudo labels.\n",
    "\n",
    "    Args:\n",
    "        df_spending: Spending-only dataframe.\n",
    "\n",
    "    Returns:\n",
    "        sklearn Pipeline predicting final_category.\n",
    "    \"\"\"\n",
    "    work = build_pseudo_category(df_spending)\n",
    "\n",
    "    X = work[[\"description\"]]\n",
    "    y = work[\"pseudo_category\"].astype(str)\n",
    "\n",
    "    model = Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(max_features=15000, ngram_range=(1, 2), min_df=2)),\n",
    "        (\"clf\", LogisticRegression(max_iter=2000, class_weight=\"balanced\"))\n",
    "    ])\n",
    "    model.fit(X[\"description\"], y)\n",
    "    return model\n",
    "\n",
    "\n",
    "def apply_spending_category_model(df_spending: pd.DataFrame, model: Pipeline) -> pd.DataFrame:\n",
    "    \"\"\"Applies category model and merges with bank categories smartly.\n",
    "\n",
    "    Policy:\n",
    "      - If bank category is informative -> keep bank category\n",
    "      - Else -> use model prediction\n",
    "\n",
    "    Args:\n",
    "        df_spending: Spending-only dataframe.\n",
    "        model: Trained classifier.\n",
    "\n",
    "    Returns:\n",
    "        Dataframe with final_category.\n",
    "    \"\"\"\n",
    "    out = df_spending.copy()\n",
    "    bank = out[\"bank_category\"].astype(str).str.lower().str.strip()\n",
    "\n",
    "    pred = model.predict(out[\"description\"].astype(str))\n",
    "    out[\"ml_category\"] = pred\n",
    "\n",
    "    out[\"final_category\"] = np.where(\n",
    "        ~bank.isin(OTHER_BANK_LABELS),\n",
    "        out[\"bank_category\"],\n",
    "        out[\"ml_category\"]\n",
    "    )\n",
    "    return out"
   ],
   "id": "4f2702d4aa956686",
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# –†–µ–≥—É–ª—è—Ä–Ω—ã–µ –ø–ª–∞—Ç–µ–∂–∏: –≤—Ä–µ–º–µ–Ω–Ω–∞—è –ø–µ—Ä–∏–æ–¥–∏—á–Ω–æ—Å—Ç—å + –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ ‚Äú–º–µ—Ä—á–∞–Ω—Ç–∞‚Äù",
   "id": "9d8c79c0c2f2997e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T14:01:30.536522Z",
     "start_time": "2026-01-13T14:01:30.530969Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def normalize_merchant_key(text: str) -> str:\n",
    "    \"\"\"Normalizes description to stable merchant key.\n",
    "\n",
    "    Args:\n",
    "        text: Raw description.\n",
    "\n",
    "    Returns:\n",
    "        Normalized merchant key.\n",
    "    \"\"\"\n",
    "    t = (text or \"\").lower()\n",
    "    t = re.sub(r\"\\d+\", \" \", t)\n",
    "    t = re.sub(r\"[^\\w\\s]\", \" \", t)\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "    # –æ–±—Ä–µ–∑–∞–µ–º, —á—Ç–æ–±—ã –Ω–µ –±—ã–ª–æ –≥–∏–≥–∞–Ω—Ç—Å–∫–∏—Ö –∫–ª—é—á–µ–π\n",
    "    return t[:80]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RecurringRule:\n",
    "    merchant_key: str\n",
    "    n: int\n",
    "    avg_amount: float\n",
    "    total_amount: float\n",
    "    period_days: float\n",
    "    period_label: str\n",
    "    last_date: pd.Timestamp\n",
    "\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "\n",
    "def detect_recurring(df_spending: pd.DataFrame, min_n: int = 3):\n",
    "    work = df_spending.copy()\n",
    "\n",
    "    # 1Ô∏è‚É£ –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –≤–µ–∫—Ç–æ—Ä –æ–ø–∏—Å–∞–Ω–∏–π\n",
    "    tfidf = TfidfVectorizer(\n",
    "        max_features=2000,\n",
    "        ngram_range=(1, 2),\n",
    "        min_df=2\n",
    "    )\n",
    "    X_text = tfidf.fit_transform(work[\"description\"].astype(str))\n",
    "\n",
    "    # 2Ô∏è‚É£ –ß–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
    "    work[\"day\"] = work[\"date\"].dt.day\n",
    "    work[\"log_amount\"] = np.log1p(work[\"amount\"].abs())\n",
    "\n",
    "    X_num = work[[\"day\", \"log_amount\"]].values\n",
    "    X_num = StandardScaler().fit_transform(X_num)\n",
    "\n",
    "    # 3Ô∏è‚É£ –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ç–µ–∫—Å—Ç + —á–∏—Å–ª–∞\n",
    "    from scipy.sparse import hstack\n",
    "    X = hstack([X_text, X_num])\n",
    "\n",
    "    # 4Ô∏è‚É£ –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è recurring-–ø–æ–≤–µ–¥–µ–Ω–∏—è\n",
    "    model = DBSCAN(\n",
    "        eps=0.6,\n",
    "        min_samples=min_n,\n",
    "        metric=\"cosine\"\n",
    "    )\n",
    "\n",
    "    labels = model.fit_predict(X)\n",
    "    work[\"recurring_cluster\"] = labels\n",
    "\n",
    "    rules = []\n",
    "    for cid in set(labels):\n",
    "        if cid == -1:\n",
    "            continue\n",
    "\n",
    "        g = work[work[\"recurring_cluster\"] == cid].sort_values(\"date\")\n",
    "\n",
    "        if len(g) < min_n:\n",
    "            continue\n",
    "\n",
    "        dates = g[\"date\"].values.astype(\"datetime64[D]\").astype(int)\n",
    "        intervals = np.diff(dates)\n",
    "\n",
    "        if len(intervals) == 0:\n",
    "            continue\n",
    "\n",
    "        med = float(np.median(intervals))\n",
    "        mad = float(np.median(np.abs(intervals - med)))\n",
    "\n",
    "        label = None\n",
    "        if mad <= 2 and 25 <= med <= 35:\n",
    "            label = \"monthly\"\n",
    "        elif mad <= 2 and 5 <= med <= 9:\n",
    "            label = \"weekly\"\n",
    "\n",
    "        if label is None:\n",
    "            continue\n",
    "\n",
    "        rules.append({\n",
    "            \"cluster\": cid,\n",
    "            \"example\": g[\"description\"].iloc[0][:60],\n",
    "            \"n\": len(g),\n",
    "            \"avg_amount\": g[\"amount\"].mean(),\n",
    "            \"total_amount\": g[\"amount\"].sum(),\n",
    "            \"period\": label\n",
    "        })\n",
    "\n",
    "    return work, pd.DataFrame(rules)\n"
   ],
   "id": "f46d775b2c2a4366",
   "outputs": [],
   "execution_count": 60
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# –ê–Ω–æ–º–∞–ª–∏–∏: –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π IsolationForest",
   "id": "758fe2756958eb3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T14:01:31.006024Z",
     "start_time": "2026-01-13T14:01:31.001550Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def detect_contextual_anomalies(df_spending, recurring_merchants, contamination=0.03):\n",
    "    work = df_spending.copy()\n",
    "    work[\"merchant_key\"] = work[\"description\"].apply(normalize_merchant_key)\n",
    "\n",
    "    work[\"is_recurring\"] = work[\"merchant_key\"].isin(recurring_merchants).astype(int)\n",
    "\n",
    "    work[\"weekday\"] = work[\"date\"].dt.weekday\n",
    "    work[\"month\"] = work[\"date\"].dt.month\n",
    "    work[\"log_abs_amount\"] = np.log1p(work[\"amount\"].abs())\n",
    "\n",
    "    anomalies = []\n",
    "\n",
    "    for flag in [0, 1]:  # non-recurring and recurring separately\n",
    "        subset = work[work[\"is_recurring\"] == flag]\n",
    "        if len(subset) < 10:\n",
    "            continue\n",
    "\n",
    "        pre = ColumnTransformer([\n",
    "            (\"num\", StandardScaler(), [\"log_abs_amount\", \"weekday\", \"month\"]),\n",
    "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), [\"final_category\"])\n",
    "        ])\n",
    "\n",
    "        model = IsolationForest(\n",
    "            contamination=0.03 if flag == 0 else 0.01,  # –ø–æ–¥–ø–∏—Å–∫–∏ —Å—Ç—Ä–æ–∂–µ\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        X = pre.fit_transform(subset[[\"log_abs_amount\", \"weekday\", \"month\", \"final_category\"]])\n",
    "        preds = model.fit_predict(X)\n",
    "        scores = model.decision_function(X)\n",
    "\n",
    "        subset = subset.copy()\n",
    "        subset[\"is_anomaly\"] = (preds == -1).astype(int)\n",
    "        subset[\"anomaly_score\"] = scores\n",
    "\n",
    "        anomalies.append(subset)\n",
    "\n",
    "    return pd.concat(anomalies)\n",
    "\n",
    "\n",
    "def filter_low_impact_anomalies(df_anomalies):\n",
    "    work = df_anomalies.copy()\n",
    "\n",
    "    medians = (\n",
    "        work.groupby(\"final_category\")[\"amount\"]\n",
    "        .apply(lambda x: x.abs().median())\n",
    "    )\n",
    "\n",
    "    work[\"impact\"] = work.apply(\n",
    "        lambda r: abs(r[\"amount\"]) / max(1, medians.get(r[\"final_category\"], 1)),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # –∞–Ω–æ–º–∞–ª–∏—è —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Ä–µ–∞–ª—å–Ω–æ –≤–ª–∏—è–µ—Ç –Ω–∞ –±—é–¥–∂–µ—Ç\n",
    "    work[\"is_anomaly\"] = np.where(\n",
    "        (work[\"is_anomaly\"] == 1) & (work[\"impact\"] > 0.5),\n",
    "        1,\n",
    "        0\n",
    "    )\n",
    "\n",
    "    return work\n",
    "\n"
   ],
   "id": "db96ccc702f9ef0b",
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# –ü–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏–π baseline + –æ—Ü–µ–Ω–∫–∞ —ç–∫–æ–Ω–æ–º–∏–∏",
   "id": "20904724b98016d6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T14:01:31.547215Z",
     "start_time": "2026-01-13T14:01:31.541610Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DEFAULT_SAVING_POTENTIAL = {\n",
    "    \"–§–∞—Å—Ç—Ñ—É–¥\": 0.35,\n",
    "    \"–†–µ—Å—Ç–æ—Ä–∞–Ω—ã –∏ –∫–∞—Ñ–µ\": 0.30,\n",
    "    \"–¢–∞–∫—Å–∏ –∏ –∫–∞—Ä—à–µ—Ä–∏–Ω–≥\": 0.30,\n",
    "    \"–°–≤—è–∑—å\": 0.15,\n",
    "    \"–ú–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å—ã\": 0.25,\n",
    "    \"–ê–ø—Ç–µ–∫–∏\": 0.08,\n",
    "    \"–î—Ä—É–≥–æ–µ\": 0.20\n",
    "}\n",
    "\n",
    "\n",
    "def monthly_category_spend(df_spending: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Builds month x category spending matrix.\n",
    "\n",
    "    Args:\n",
    "        df_spending: Spending-only dataframe with final_category.\n",
    "\n",
    "    Returns:\n",
    "        Pivot table with absolute spending.\n",
    "    \"\"\"\n",
    "    work = df_spending.copy()\n",
    "    work[\"month\"] = work[\"date\"].dt.to_period(\"M\").astype(str)\n",
    "    piv = (\n",
    "        work.groupby([\"month\", \"final_category\"])[\"amount\"]\n",
    "        .sum()\n",
    "        .abs()\n",
    "        .unstack(fill_value=0.0)\n",
    "    )\n",
    "    return piv\n",
    "\n",
    "\n",
    "def robust_baseline(piv):\n",
    "    low = piv.quantile(0.3)\n",
    "    high = piv.quantile(0.7)\n",
    "\n",
    "    mask = (piv >= low) & (piv <= high)\n",
    "    trimmed = piv.where(mask)\n",
    "\n",
    "    return trimmed.mean(axis=0)\n",
    "\n",
    "\n",
    "def estimate_savings_plan(\n",
    "        df_spending: pd.DataFrame,\n",
    "        recurring_df: pd.DataFrame\n",
    ") -> Tuple[float, pd.DataFrame, List[str]]:\n",
    "    \"\"\"Estimates potential savings based on baseline overspending.\n",
    "\n",
    "    Args:\n",
    "        df_spending: Spending-only dataframe.\n",
    "        recurring_df: Recurring payments summary frame.\n",
    "\n",
    "    Returns:\n",
    "        (total_savings, per_category_plan, explanations)\n",
    "    \"\"\"\n",
    "    piv = monthly_category_spend(df_spending)\n",
    "    if piv.shape[0] < 2:\n",
    "        return 0.0, pd.DataFrame(), [\"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –º–µ—Å—è—Ü–µ–≤ –¥–ª—è —É—Å—Ç–æ–π—á–∏–≤–æ–π –æ—Ü–µ–Ω–∫–∏ baseline.\"]\n",
    "\n",
    "    baseline = robust_baseline(piv)\n",
    "    overspend = (piv - baseline).clip(lower=0.0)\n",
    "\n",
    "    # –æ–±—â–∏–π –ø–µ—Ä–µ—Ä–∞—Å—Ö–æ–¥ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º –∑–∞ –≤—Å–µ –º–µ—Å—è—Ü—ã\n",
    "    overspend_total = overspend.sum(axis=0).sort_values(ascending=False)\n",
    "\n",
    "    plan_rows = []\n",
    "    explanations: List[str] = []\n",
    "\n",
    "    for cat, extra in overspend_total.items():\n",
    "        if extra <= 0:\n",
    "            continue\n",
    "        coef = DEFAULT_SAVING_POTENTIAL.get(str(cat), DEFAULT_SAVING_POTENTIAL[\"–î—Ä—É–≥–æ–µ\"])\n",
    "        save = float(extra * coef)\n",
    "        plan_rows.append((str(cat), float(extra), coef, save))\n",
    "\n",
    "    plan = pd.DataFrame(plan_rows, columns=[\"category\", \"overspend_total\", \"saving_coef\", \"potential_saving\"])\n",
    "\n",
    "    # –æ—Ç–¥–µ–ª—å–Ω–∞—è –∞–∫–∫—É—Ä–∞—Ç–Ω–∞—è —Å—Ç—Ä–æ–∫–∞ –ø–æ –ø–æ–¥–ø–∏—Å–∫–∞–º/—Ä–µ–≥—É–ª—è—Ä–Ω—ã–º monthly\n",
    "    if len(recurring_df) > 0:\n",
    "        monthly = recurring_df[recurring_df[\"period_label\"] == \"monthly\"].copy()\n",
    "        if len(monthly) > 0:\n",
    "            subs_total = float(monthly[\"total_amount\"].abs().sum())\n",
    "            # —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ: 10-30% (–ø–µ—Ä–µ—Ö–æ–¥ –Ω–∞ —Ç–∞—Ä–∏—Ñ, –æ—Ç–º–µ–Ω–∞ —á–∞—Å—Ç–∏)\n",
    "            subs_save = subs_total * 0.20\n",
    "            explanations.append(\n",
    "                f\"–†–µ–≥—É–ª—è—Ä–Ω—ã–µ monthly-–ø–ª–∞—Ç–µ–∂–∏: –≤—Å–µ–≥–æ {subs_total:,.0f} ‚ÇΩ, —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è ~{subs_save:,.0f} ‚ÇΩ.\"\n",
    "            )\n",
    "\n",
    "    total_savings = float(plan[\"potential_saving\"].sum()) if len(plan) else 0.0\n",
    "\n",
    "    # –∫—Ä–∞—Ç–∫–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ —Ç–æ–ø-—Ç–æ—á–µ–∫\n",
    "    if len(plan) > 0:\n",
    "        top = plan.sort_values(\"potential_saving\", ascending=False).head(3)\n",
    "        for _, r in top.iterrows():\n",
    "            explanations.append(\n",
    "                f\"–ö–∞—Ç–µ–≥–æ—Ä–∏—è '{r['category']}': –ø–µ—Ä–µ—Ä–∞—Å—Ö–æ–¥ {r['overspend_total']:,.0f} ‚ÇΩ, \"\n",
    "                f\"–ø–æ—Ç–µ–Ω—Ü–∏–∞–ª —ç–∫–æ–Ω–æ–º–∏–∏ ~{r['potential_saving']:,.0f} ‚ÇΩ.\"\n",
    "            )\n",
    "\n",
    "    return round(total_savings, 2), plan.sort_values(\"potential_saving\", ascending=False), explanations\n"
   ],
   "id": "c36c093505f417e",
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# –û—Ç—á—ë—Ç",
   "id": "c35f40302181e5a9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T14:01:32.237888Z",
     "start_time": "2026-01-13T14:01:32.229878Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def format_money(x: float) -> str:\n",
    "    \"\"\"Formats money with thousands separator.\"\"\"\n",
    "    return f\"{x:,.0f} ‚ÇΩ\".replace(\",\", \" \")\n",
    "\n",
    "\n",
    "def build_user_report(\n",
    "        df_all: pd.DataFrame,\n",
    "        df_spending: pd.DataFrame,\n",
    "        recurring_df: pd.DataFrame,\n",
    "        anomalies_df: pd.DataFrame,\n",
    "        savings_total: float,\n",
    "        savings_plan: pd.DataFrame,\n",
    "        savings_notes: List[str],\n",
    "        cashflow: Dict[str, float],\n",
    "        top_n: int = 8\n",
    ") -> str:\n",
    "    \"\"\"Builds Telegram-ready report text.\n",
    "\n",
    "    Args:\n",
    "        df_all: All transactions with transaction_type.\n",
    "        df_spending: Spending-only with final_category.\n",
    "        recurring_df: Summary of recurring payments.\n",
    "        anomalies_df: Spending dataframe with anomaly flags.\n",
    "        savings_total: Total potential savings.\n",
    "        savings_plan: Per-category plan.\n",
    "        savings_notes: Explanation bullet points.\n",
    "        top_n: Number of categories to show.\n",
    "\n",
    "    Returns:\n",
    "        Report string.\n",
    "    \"\"\"\n",
    "    lines: List[str] = []\n",
    "\n",
    "    # –ø–µ—Ä–∏–æ–¥\n",
    "    d0 = df_all[\"date\"].min().date()\n",
    "    d1 = df_all[\"date\"].max().date()\n",
    "    lines.append(f\"üìÖ –ü–µ—Ä–∏–æ–¥: {d0} ‚Äî {d1}\\n\")\n",
    "    lines.append(\"üí∞ –†–µ–∞–ª—å–Ω—ã–π –±—é–¥–∂–µ—Ç —ç—Ç–æ–π –∫–∞—Ä—Ç—ã:\")\n",
    "    lines.append(f\"- –ü–æ—Å—Ç—É–ø–∏–ª–æ –Ω–∞ –∫–∞—Ä—Ç—É: {format_money(cashflow['inflow'])}\")\n",
    "    lines.append(f\"- –ü–æ—Ç—Ä–∞—á–µ–Ω–æ: {format_money(cashflow['spending'])}\")\n",
    "    lines.append(f\"- –ü–µ—Ä–µ–≤–µ–¥–µ–Ω–æ –Ω–∞ –¥—Ä—É–≥–∏–µ —Å—á–µ—Ç–∞: {format_money(cashflow['transfer_out'])}\")\n",
    "    lines.append(\"\")\n",
    "\n",
    "    # —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø–æ—Ç–æ–∫–æ–≤\n",
    "    flows = df_all.groupby(\"transaction_type\")[\"amount\"].sum()\n",
    "    lines.append(\"üí≥ –î–µ–Ω–µ–∂–Ω—ã–µ –ø–æ—Ç–æ–∫–∏ (–º–æ–¥–µ–ª—å —Ç–∏–ø–∞ –æ–ø–µ—Ä–∞—Ü–∏–π):\")\n",
    "    for k in [\"income\", \"refund\", \"transfer\", \"spending\"]:\n",
    "        if k in flows.index:\n",
    "            lines.append(f\"- {k}: {format_money(float(flows[k]))}\")\n",
    "    lines.append(\"\")\n",
    "\n",
    "    # –∫—É–¥–∞ —É—Ö–æ–¥—è—Ç –¥–µ–Ω—å–≥–∏ (—Ç–æ–ª—å–∫–æ spending)\n",
    "    cat = (\n",
    "        df_spending.groupby(\"final_category\")[\"amount\"]\n",
    "        .sum().abs().sort_values(ascending=False)\n",
    "    )\n",
    "    total_spend = float(cat.sum()) if len(cat) else 0.0\n",
    "    lines.append(\"üìä –ö—É–¥–∞ —É—Ö–æ–¥—è—Ç –¥–µ–Ω—å–≥–∏ (—Ç–æ–ª—å–∫–æ —Ä–∞—Å—Ö–æ–¥—ã):\")\n",
    "    for c, v in cat.head(top_n).items():\n",
    "        share = (float(v) / total_spend * 100) if total_spend > 0 else 0\n",
    "        lines.append(f\"- {c}: {format_money(float(v))} ({share:.1f}%)\")\n",
    "    if len(cat) > top_n:\n",
    "        rest = float(cat.iloc[top_n:].sum())\n",
    "        lines.append(f\"- –î—Ä—É–≥–æ–µ (–æ—Å—Ç–∞–ª—å–Ω—ã–µ): {format_money(rest)}\")\n",
    "    lines.append(\"\")\n",
    "\n",
    "    # —Ä–µ–≥—É–ª—è—Ä–Ω—ã–µ\n",
    "    lines.append(\"üîÅ –†–µ–≥—É–ª—è—Ä–Ω—ã–µ –ø–ª–∞—Ç–µ–∂–∏ (–ø–æ –ø–µ—Ä–∏–æ–¥–∏—á–Ω–æ—Å—Ç–∏):\")\n",
    "    if len(recurring_df) == 0:\n",
    "        lines.append(\"- –ù–µ –Ω–∞–π–¥–µ–Ω–æ —É—Å—Ç–æ–π—á–∏–≤—ã—Ö weekly/monthly —à–∞–±–ª–æ–Ω–æ–≤.\")\n",
    "    else:\n",
    "        show = recurring_df.sort_values(\"total_amount\").head(10)\n",
    "        for _, r in show.iterrows():\n",
    "            lines.append(\n",
    "                f\"- {r['period']} | {r['example']}‚Ä¶ \"\n",
    "                f\"({int(r['n'])} —Ä–∞–∑, —Å—Ä–µ–¥–Ω–µ–µ {format_money(abs(float(r['avg_amount'])))}; \"\n",
    "                f\"–≤—Å–µ–≥–æ {format_money(abs(float(r['total_amount'])))} )\"\n",
    "            )\n",
    "\n",
    "    lines.append(\"\")\n",
    "\n",
    "    # –∞–Ω–æ–º–∞–ª–∏–∏\n",
    "    lines.append(\"‚ö†Ô∏è –ù–µ—Ç–∏–ø–∏—á–Ω—ã–µ —Ç—Ä–∞—Ç—ã (–∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∞—è –º–æ–¥–µ–ª—å):\")\n",
    "    anoms = anomalies_df[anomalies_df[\"is_anomaly\"] == 1].copy()\n",
    "    if len(anoms) == 0:\n",
    "        lines.append(\"- –ê–Ω–æ–º–∞–ª–∏–π –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ.\")\n",
    "    else:\n",
    "        # —Å–∞–º—ã–µ \"–Ω–∏–∑–∫–∏–µ\" anomaly_score\n",
    "        anoms = anoms.sort_values(\"anomaly_score\").head(8)\n",
    "        for _, r in anoms.iterrows():\n",
    "            lines.append(\n",
    "                f\"- {r['date'].date()} | {r['final_category']} | {r['description'][:45]}‚Ä¶ ‚Üí {format_money(abs(float(r['amount'])))}\"\n",
    "            )\n",
    "    lines.append(\"\")\n",
    "\n",
    "    # —ç–∫–æ–Ω–æ–º–∏—è\n",
    "    lines.append(\"üí∞ –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª —ç–∫–æ–Ω–æ–º–∏–∏ (baseline + –ø–ª–∞–Ω):\")\n",
    "    lines.append(f\"- –û—Ü–µ–Ω–∫–∞: ~{format_money(abs(float(savings_total)))} –∑–∞ –ø–µ—Ä–∏–æ–¥.\")\n",
    "    for note in savings_notes[:5]:\n",
    "        lines.append(f\"- {note}\")\n",
    "    if len(savings_plan) > 0:\n",
    "        lines.append(\"\\n–ü–ª–∞–Ω –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º (—Ç–æ–ø-5):\")\n",
    "        for _, r in savings_plan.head(5).iterrows():\n",
    "            lines.append(\n",
    "                f\"- {r['category']}: –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª ~{format_money(float(r['potential_saving']))} \"\n",
    "                f\"(–∫–æ—ç—Ñ. {int(float(r['saving_coef']) * 100)}%)\"\n",
    "            )\n",
    "\n",
    "    return \"\\n\".join(lines)\n"
   ],
   "id": "3e5b52bb6dfa2216",
   "outputs": [],
   "execution_count": 63
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# –ò—Ç–æ–≥–æ–≤—ã–π Pipeline",
   "id": "eb097f830c168d57"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T14:01:33.073072Z",
     "start_time": "2026-01-13T14:01:33.068061Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_pipeline(path: str) -> Dict[str, object]:\n",
    "    \"\"\"Runs full analytics pipeline.\n",
    "\n",
    "    Args:\n",
    "        path: Path to bank export file (CSV/XLSX).\n",
    "\n",
    "    Returns:\n",
    "        Dict with report and intermediate artifacts.\n",
    "    \"\"\"\n",
    "    # A) load + normalize\n",
    "    raw = load_bank_file(path)\n",
    "    df = normalize_columns(raw)\n",
    "\n",
    "    # B) transaction type model\n",
    "    type_model = train_transaction_type_model(df)\n",
    "    df = apply_transaction_type_model(df, type_model)\n",
    "\n",
    "    df[\"direction\"] = np.where(df[\"amount\"] > 0, \"in\", \"out\")\n",
    "\n",
    "    # –í—Å–µ –≤—Ö–æ–¥—è—â–∏–µ –¥–µ–Ω—å–≥–∏ –Ω–∞ —ç—Ç—É –∫–∞—Ä—Ç—É\n",
    "    inflow = df[df[\"amount\"] > 0][\"amount\"].sum()\n",
    "\n",
    "    # –í—Å–µ —Ä–µ–∞–ª—å–Ω—ã–µ —Ç—Ä–∞—Ç—ã (–Ω–µ –ø–µ—Ä–µ–≤–æ–¥—ã)\n",
    "    spending_out = df[df[\"transaction_type\"] == \"spending\"][\"amount\"].abs().sum()\n",
    "\n",
    "    # –ü–µ—Ä–µ–≤–æ–¥—ã —Å —ç—Ç–æ–π –∫–∞—Ä—Ç—ã –Ω–∞—Ä—É–∂—É\n",
    "    transfer_out = df[(df[\"transaction_type\"] == \"transfer\") & (df[\"amount\"] < 0)][\"amount\"].abs().sum()\n",
    "\n",
    "    # –†–µ–∞–ª—å–Ω—ã–π –¥–æ—Å—Ç—É–ø–Ω—ã–π –±—é–¥–∂–µ—Ç —ç—Ç–æ–π –∫–∞—Ä—Ç—ã\n",
    "    available_budget = inflow\n",
    "\n",
    "    # –î–ª—è –æ—Ç—á—ë—Ç–∞\n",
    "    cashflow = {\n",
    "        \"inflow\": float(inflow),\n",
    "        \"spending\": float(spending_out),\n",
    "        \"transfer_out\": float(transfer_out),\n",
    "        \"available_budget\": float(available_budget)\n",
    "    }\n",
    "\n",
    "    # C) spending-only\n",
    "    df_sp = df[df[\"transaction_type\"] == \"spending\"].copy()\n",
    "    if len(df_sp) == 0:\n",
    "        return {\"report\": \"–ù–µ –Ω–∞–π–¥–µ–Ω–æ –æ–ø–µ—Ä–∞—Ü–∏–π —Ä–∞—Å—Ö–æ–¥–æ–≤ (spending). –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –≤—ã–≥—Ä—É–∑–∫—É/–∑–Ω–∞–∫ —Å—É–º–º.\", \"df\": df}\n",
    "\n",
    "    # category model\n",
    "    cat_model = train_spending_category_model(df_sp)\n",
    "    df_sp = apply_spending_category_model(df_sp, cat_model)\n",
    "\n",
    "    # D) recurring (time-based)\n",
    "    df_sp_keyed, recurring_df = detect_recurring(df_sp, min_n=4)\n",
    "\n",
    "    # recurring_df —Ç–µ–ø–µ—Ä—å —É–∂–µ DataFrame\n",
    "    if len(recurring_df) > 0:\n",
    "        recurring_merchants = set(\n",
    "            df_sp_keyed[df_sp_keyed[\"recurring_cluster\"] != -1][\"merchant_key\"].astype(str)\n",
    "        )\n",
    "    else:\n",
    "        recurring_merchants = set()\n",
    "\n",
    "\n",
    "    # E) anomalies (contextual)\n",
    "    df_an = detect_contextual_anomalies(df_sp_keyed, recurring_merchants=recurring_merchants, contamination=0.03)\n",
    "    df_an = filter_low_impact_anomalies(df_an)\n",
    "\n",
    "    # F) savings\n",
    "    savings_total, savings_plan, savings_notes = estimate_savings_plan(df_sp_keyed, recurring_df)\n",
    "\n",
    "    # report\n",
    "    report = build_user_report(\n",
    "        df_all=df,\n",
    "        df_spending=df_sp_keyed,\n",
    "        recurring_df=recurring_df,\n",
    "        anomalies_df=df_an,\n",
    "        savings_total=savings_total,\n",
    "        savings_plan=savings_plan,\n",
    "        savings_notes=savings_notes,\n",
    "        cashflow=cashflow\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"report\": report,\n",
    "        \"df_all\": df,\n",
    "        \"df_spending\": df_sp_keyed,\n",
    "        \"recurring\": recurring_df,\n",
    "        \"anomalies\": df_an[df_an[\"is_anomaly\"] == 1].copy(),\n",
    "        \"savings_total\": savings_total,\n",
    "        \"savings_plan\": savings_plan,\n",
    "        \"type_model\": type_model,\n",
    "        \"category_model\": cat_model,\n",
    "    }\n"
   ],
   "id": "d4d1fd118acde325",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T14:01:33.796164Z",
     "start_time": "2026-01-13T14:01:33.475102Z"
    }
   },
   "cell_type": "code",
   "source": [
    "res = run_pipeline(\"assets/12 –∏—é–ª—è 2025 - 11 —è–Ω–≤–∞—Ä—è 2026.xlsx\")\n",
    "print(res[\"report\"])"
   ],
   "id": "af769bac5639a198",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÖ –ü–µ—Ä–∏–æ–¥: 2025-07-13 ‚Äî 2026-01-11\n",
      "\n",
      "üí∞ –†–µ–∞–ª—å–Ω—ã–π –±—é–¥–∂–µ—Ç —ç—Ç–æ–π –∫–∞—Ä—Ç—ã:\n",
      "- –ü–æ—Å—Ç—É–ø–∏–ª–æ –Ω–∞ –∫–∞—Ä—Ç—É: 9 421 ‚ÇΩ\n",
      "- –ü–æ—Ç—Ä–∞—á–µ–Ω–æ: 66 257 ‚ÇΩ\n",
      "- –ü–µ—Ä–µ–≤–µ–¥–µ–Ω–æ –Ω–∞ –¥—Ä—É–≥–∏–µ —Å—á–µ—Ç–∞: 140 457 ‚ÇΩ\n",
      "\n",
      "üí≥ –î–µ–Ω–µ–∂–Ω—ã–µ –ø–æ—Ç–æ–∫–∏ (–º–æ–¥–µ–ª—å —Ç–∏–ø–∞ –æ–ø–µ—Ä–∞—Ü–∏–π):\n",
      "- income: 9 421 ‚ÇΩ\n",
      "- transfer: -140 457 ‚ÇΩ\n",
      "- spending: -66 257 ‚ÇΩ\n",
      "\n",
      "üìä –ö—É–¥–∞ —É—Ö–æ–¥—è—Ç –¥–µ–Ω—å–≥–∏ (—Ç–æ–ª—å–∫–æ —Ä–∞—Å—Ö–æ–¥—ã):\n",
      "- –¢–∞–∫—Å–∏ –∏ –∫–∞—Ä—à–µ—Ä–∏–Ω–≥: 28 852 ‚ÇΩ (43.5%)\n",
      "- –¢–µ–ª–µ—Ñ–æ–Ω, –∏–Ω—Ç–µ—Ä–Ω–µ—Ç, –¢–í: 8 206 ‚ÇΩ (12.4%)\n",
      "- –§–∞—Å—Ç—Ñ—É–¥: 6 316 ‚ÇΩ (9.5%)\n",
      "- –î—Ä—É–≥–æ–µ: 5 401 ‚ÇΩ (8.2%)\n",
      "- –¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç: 5 157 ‚ÇΩ (7.8%)\n",
      "- –ê–ø—Ç–µ–∫–∏: 3 005 ‚ÇΩ (4.5%)\n",
      "- –ú—É–ª—å—Ç–∏–º–µ–¥–∏–∞: 1 823 ‚ÇΩ (2.8%)\n",
      "- –ö–∞—Ñ–µ –∏ —Ä–µ—Å—Ç–æ—Ä–∞–Ω—ã: 1 690 ‚ÇΩ (2.6%)\n",
      "- –î—Ä—É–≥–æ–µ (–æ—Å—Ç–∞–ª—å–Ω—ã–µ): 5 807 ‚ÇΩ\n",
      "\n",
      "üîÅ –†–µ–≥—É–ª—è—Ä–Ω—ã–µ –ø–ª–∞—Ç–µ–∂–∏ (–ø–æ –ø–µ—Ä–∏–æ–¥–∏—á–Ω–æ—Å—Ç–∏):\n",
      "- –ù–µ –Ω–∞–π–¥–µ–Ω–æ —É—Å—Ç–æ–π—á–∏–≤—ã—Ö weekly/monthly —à–∞–±–ª–æ–Ω–æ–≤.\n",
      "\n",
      "‚ö†Ô∏è –ù–µ—Ç–∏–ø–∏—á–Ω—ã–µ —Ç—Ä–∞—Ç—ã (–∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∞—è –º–æ–¥–µ–ª—å):\n",
      "- –ê–Ω–æ–º–∞–ª–∏–π –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ.\n",
      "\n",
      "üí∞ –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª —ç–∫–æ–Ω–æ–º–∏–∏ (baseline + –ø–ª–∞–Ω):\n",
      "- –û—Ü–µ–Ω–∫–∞: ~10 526 ‚ÇΩ –∑–∞ –ø–µ—Ä–∏–æ–¥.\n",
      "- –ö–∞—Ç–µ–≥–æ—Ä–∏—è '–¢–∞–∫—Å–∏ –∏ –∫–∞—Ä—à–µ—Ä–∏–Ω–≥': –ø–µ—Ä–µ—Ä–∞—Å—Ö–æ–¥ 22,770 ‚ÇΩ, –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª —ç–∫–æ–Ω–æ–º–∏–∏ ~6,831 ‚ÇΩ.\n",
      "- –ö–∞—Ç–µ–≥–æ—Ä–∏—è '–§–∞—Å—Ç—Ñ—É–¥': –ø–µ—Ä–µ—Ä–∞—Å—Ö–æ–¥ 3,337 ‚ÇΩ, –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª —ç–∫–æ–Ω–æ–º–∏–∏ ~1,168 ‚ÇΩ.\n",
      "- –ö–∞—Ç–µ–≥–æ—Ä–∏—è '–ö–∞—Ñ–µ –∏ —Ä–µ—Å—Ç–æ—Ä–∞–Ω—ã': –ø–µ—Ä–µ—Ä–∞—Å—Ö–æ–¥ 1,690 ‚ÇΩ, –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª —ç–∫–æ–Ω–æ–º–∏–∏ ~338 ‚ÇΩ.\n",
      "\n",
      "–ü–ª–∞–Ω –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º (—Ç–æ–ø-5):\n",
      "- –¢–∞–∫—Å–∏ –∏ –∫–∞—Ä—à–µ—Ä–∏–Ω–≥: –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª ~6 831 ‚ÇΩ (–∫–æ—ç—Ñ. 30%)\n",
      "- –§–∞—Å—Ç—Ñ—É–¥: –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª ~1 168 ‚ÇΩ (–∫–æ—ç—Ñ. 35%)\n",
      "- –ö–∞—Ñ–µ –∏ —Ä–µ—Å—Ç–æ—Ä–∞–Ω—ã: –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª ~338 ‚ÇΩ (–∫–æ—ç—Ñ. 20%)\n",
      "- –û–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ: –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª ~303 ‚ÇΩ (–∫–æ—ç—Ñ. 20%)\n",
      "- –ú—É–ª—å—Ç–∏–º–µ–¥–∏–∞: –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª ~290 ‚ÇΩ (–∫–æ—ç—Ñ. 20%)\n"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "19c63332171a8070"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
